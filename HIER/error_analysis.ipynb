{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
    "import pickle as pkl, random\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import gc\n",
    "import os, sys\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import logging\n",
    "from nltk.util import ngrams\n",
    "import re, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from model import *\n",
    "from metrics import *\n",
    "from collections import OrderedDict\n",
    "from evaluate import evaluateModel, evaluateModel_Slow\n",
    "import Constants\n",
    "import argparse\n",
    "\n",
    "if not os.path.isdir('running'):\n",
    "    os.makedirs('running')\n",
    "\n",
    "\n",
    "def split_to_files(split):\n",
    "    if split=='train':\n",
    "        return train_dialog_files\n",
    "    if split=='val':\n",
    "        return val_dialog_files\n",
    "    if split=='test':\n",
    "        return test_dialog_files\n",
    "    return None\n",
    "\n",
    "\n",
    "def split_to_responses(split): # return original responses\n",
    "    if split=='train':\n",
    "        return train_responses\n",
    "    if split=='val':\n",
    "        return val_responses\n",
    "    if split=='test':\n",
    "        return test_responses\n",
    "    return ValueError\n",
    "    \n",
    "\n",
    "def train_epoch(model, epoch, batch_size, criterion, optimizer, scheduler): # losses per batch\n",
    "    model.train()\n",
    "    total_loss =0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(idxtoword)\n",
    "    nbatches = len(train)//batch_size\n",
    "    \n",
    "#     if torch.cuda.is_available():\n",
    "#         stat_cuda('before epoch')\n",
    "        \n",
    "    score=0\n",
    "    total_bleu_score=0\n",
    "    accumulated_steps = 3\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (data, targets, labels, act_vecs) in tqdm(enumerate(data_loader_acts(train, train_counter, train_hierarchial_actvecs, batch_size, wordtoidx)), total=nbatches):\n",
    "\n",
    "        batch_size_curr = data.shape[1]\n",
    "        # optimizer.zero_grad()             \n",
    "\n",
    "        output = model(data, targets, act_vecs)\n",
    "\n",
    "        cur_loss = criterion(output.view(-1, ntokens), labels.reshape(-1))\n",
    "            \n",
    "        loss = cur_loss / accumulated_steps\n",
    "        loss.backward()\n",
    "    \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        if i%accumulated_steps==0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += cur_loss.item()*batch_size_curr\n",
    "        elapsed = time.time()-start_time\n",
    "\n",
    "    total_loss /= len(train)\n",
    "    logger.debug('==> Epoch {}, Train \\tLoss: {:0.5f}\\tTime taken: {:0.1f}'.format(epoch,  total_loss, elapsed))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, args, dataset, dataset_counter, dataset_act_vecs, batch_size, criterion, split, method='beam', beam_size=None):\n",
    "    batch_size = args.batch_size\n",
    "    if args.batch_size != 1:\n",
    "        raise Exception(\"Batch size needs to 1 for error analysis.\")\n",
    "\n",
    "    logger.debug('{} search {}'.format(method, split))\n",
    "    if method=='beam':\n",
    "        logger.debug('Beam size {}'.format(beam_size))\n",
    "    model.eval()\n",
    "    total_loss =0\n",
    "    ntokens = len(wordtoidx)\n",
    "    score=0\n",
    "    start = time.time()\n",
    "    nbatches = len(dataset)//batch_size\n",
    "    \n",
    "    losses = []\n",
    "    bleus = []\n",
    "    f1_ents = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, targets, labels, act_vecs) in tqdm(enumerate(data_loader_acts(dataset, dataset_counter, dataset_act_vecs, batch_size, wordtoidx)) , total=len(dataset)//batch_size):\n",
    "#             if i > 100:\n",
    "#               break\n",
    "            batch_size_curr = targets.shape[1]\n",
    "            # assert(data.shape[1]==act_vecs.shape[1])\n",
    "            # act_vecs is 44,bs\n",
    "\n",
    "            if method=='beam':\n",
    "                if isinstance(model, nn.DataParallel):\n",
    "                    # gives list of sentences itself\n",
    "                    output = model.module.translate_batch(data, act_vecs, beam_size, batch_size_curr)\n",
    "                else:\n",
    "                    output = model.translate_batch(data, act_vecs, beam_size , batch_size_curr) \n",
    "            elif method=='greedy':\n",
    "                if isinstance(model, nn.DataParallel):\n",
    "                    output, output_max = model.module.greedy_search(data,act_vecs,  batch_size_curr) # .module. if using dataparallel\n",
    "                else:\n",
    "                    output, output_max = model.greedy_search(data,act_vecs, batch_size_curr) \n",
    "\n",
    "\n",
    "            label_pad_mask = labels.transpose(0,1)!=0\n",
    "            \n",
    "            if torch.is_tensor(output): # greedy search\n",
    "                _L = criterion(output.view(-1, ntokens), labels.reshape(-1)).item()*batch_size_curr\n",
    "                total_loss += _L\n",
    "                losses.append(_L)\n",
    "                # output = torch.max(output, dim=2)[1]\n",
    "                output_max = post_process(output_max.transpose(0,1))    \n",
    "                \n",
    "                if i==0:\n",
    "                    hyp = output_max\n",
    "                    ref = targets.transpose(0,1)\n",
    "                else:\n",
    "                    hyp = torch.cat((hyp, output_max), dim=0)\n",
    "                    ref= torch.cat((ref, targets.transpose(0,1)), dim=0)\n",
    "            else: # beam search\n",
    "                if i==0:\n",
    "                    hyp = [torch.tensor(l) for l in output]\n",
    "                    ref = targets.transpose(0,1)\n",
    "                else:\n",
    "                    hyp.extend([torch.tensor(l) for l in output])\n",
    "                    ref= torch.cat((ref, targets.transpose(0,1)), dim=0)\n",
    "            # print(hyp.shape)\n",
    "            # print(ref.shape)\n",
    "            _h = tensor_to_sents(output_max , wordtoidx)\n",
    "            _r = tensor_to_sents(targets.transpose(0,1), wordtoidx)\n",
    "            # print(_h)  # hyp[indices]\n",
    "            # print(_r)\n",
    "            _bleu = BLEU_calc.score(_h, _r, wordtoidx)*100\n",
    "            _f1 = F1_calc.score(_h, _r, wordtoidx)*100\n",
    "            bleus.append(_bleu)\n",
    "            f1_ents.append(_f1)\n",
    "            # print(f\"BLEU: {_bleu}, F1-Entity: {_f1}\")\n",
    "            \n",
    "            # evaluate_dials = {}\n",
    "            # evaluate_dials[all_dialog_files[i]]=[_h]\n",
    "            \n",
    "            # _match, _succ = evaluateModel(evaluate_dials)\n",
    "            # print(f\"match: {_match}, success: {_succ}\")\n",
    "            \n",
    "\n",
    "        # calculation for bleu scores of different context lengths\n",
    "    #    limit_small = len(dataset)//3 \n",
    "    #    limit_med = 2*len(dataset)//3 \n",
    "    #    score_small = BLEU_calc.score(hyp[:limit_small], ref[:limit_small], wordtoidx) * 100\n",
    "    #    score_medium = BLEU_calc.score(hyp[limit_small:limit_med], ref[limit_small:limit_med], wordtoidx)* 100\n",
    "    #    score_large = BLEU_calc.score(hyp[limit_med:], ref[limit_med:], wordtoidx)* 100\n",
    "\n",
    "    #    logger.debug('BLEU Scores for different buckets: ')\n",
    "    #    logger.debug('Small: {} \\tMedium: {}\\tLarge: {}'.format(score_small, score_medium, score_large))\n",
    "\n",
    "        indices = list(range(0, len(dataset)))\n",
    "        # indices = list(range(0, args.batch_size)) # uncomment this to run for one batch\n",
    "\n",
    "        pred_hyp = tensor_to_sents(hyp , wordtoidx)  # hyp[indices]\n",
    "        pred_ref = split_to_responses(split)\n",
    "        # pred_ref = split_to_responses(split)[:args.batch_size] # uncomment for 1 batch\n",
    "        \n",
    "        score = BLEU_calc.score(pred_hyp, pred_ref, wordtoidx)*100\n",
    "        f1_entity = F1_calc.score(pred_hyp, pred_ref, wordtoidx)*100\n",
    "        total_loss = total_loss/len(dataset)\n",
    "\n",
    "        all_dialog_files = split_to_files(split)\n",
    "        evaluate_dials = {}\n",
    "        for i, h in enumerate(pred_hyp):\n",
    "            if all_dialog_files[i] in evaluate_dials:\n",
    "                evaluate_dials[all_dialog_files[i]].append(h)\n",
    "            else:\n",
    "                evaluate_dials[all_dialog_files[i]]=[h]\n",
    "\n",
    "        matches, successes, all_match_success = evaluateModel_Slow(evaluate_dials) # gives matches(inform), success\n",
    "        \n",
    "        data, _, _ = name_to_dataset(split)\n",
    "        \n",
    "        # Readable outputs\n",
    "        if method=='beam':\n",
    "            pred_file = open(args.log_path+'pred_beam_'+str(beam_size)+'_'+split+'.txt', 'w')\n",
    "        elif method=='greedy':\n",
    "            pred_file = open(args.log_path+'pred_greedy_'+split+'.txt', 'w')\n",
    "\n",
    "        pred_file.write('\\n\\n***'+split+'***')\n",
    "        for idx, h, r, l, b, f1 in tqdm(zip(indices, pred_hyp, pred_ref, losses, bleus, f1_ents)):\n",
    "            pred_file.write('\\n\\nContext: \\n'+str('\\n'.join(data[idx][:-1])))\n",
    "            pred_file.write('\\nGold sentence: '+str(r)+'\\nOutput: '+str(h))\n",
    "            pred_file.write('\\nLoss: '+str(l))\n",
    "            pred_file.write(f\"\\nBLEU: {b}, F1-Entity: {f1}\")\n",
    "        \n",
    "        # Parsable Outputs\n",
    "        if method=='beam':\n",
    "            pred_file = open(args.log_path+'error_beam_'+str(beam_size)+'_'+split+'.tsv', 'w')\n",
    "        elif method=='greedy':\n",
    "            pred_file = open(args.log_path+'error_greedy_'+split+'.tsv', 'w')\n",
    "\n",
    "        pred_file.write(f\"file\\tcontext\\tgold\\tgenerated\\tloss\\tbleu\\tf1_entity\\n\")\n",
    "        for idx, h, r, l, b, f1, file in tqdm(zip(indices, pred_hyp, pred_ref, losses, bleus, f1_ents, all_dialog_files)):\n",
    "            pred_file.write(f\"{file}\\t{'<br>'.join(data[idx][:-1])}\\t{str(r)}\\t{str(h)}\\t{str(l)}\\t{str(b)}\\t{str(f1)}\\n\")\n",
    "        \n",
    "        # Match and Success stats\n",
    "        if method=='beam':\n",
    "            pred_file = open(args.log_path+'stats_beam_'+str(beam_size)+'_'+split+'.tsv', 'w')\n",
    "        elif method=='greedy':\n",
    "            pred_file = open(args.log_path+'stats_greedy_'+split+'.tsv', 'w')\n",
    "            \n",
    "        pred_file.write(f\"file\\tmatch\\tsuccess\\n\")\n",
    "        for idx, (_match, _succ) in tqdm(all_match_success.items()):\n",
    "            pred_file.write(f\"{idx}\\t{str(_match)}\\t{str(_succ)}\\n\")\n",
    "\n",
    "    elapsed = time.time()-start\n",
    "    criteria = score+0.5*(matches+successes)\n",
    "    logger.debug('==> {} \\tLoss: {:0.4f}\\tBleu: {:0.3f}\\tF1-Entity {:0.3f}\\tInform {:0.3f}\\tSuccesses: {:0.3f}\\tCriteria: {:0.3f}\\tTime taken: {:0.1f}s'.format( split, total_loss, score, f1_entity, matches, successes, criteria, elapsed))\n",
    "    return total_loss, score, f1_entity, matches, successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_nograd(model, epoch, batch_size,criterion, split): # losses per batch\n",
    "    model.eval()\n",
    "    total_loss =0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(idxtoword)\n",
    "    \n",
    "    dataset, dataset_counter, dataset_act_vecs = name_to_dataset(split)    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, targets, labels, act_vecs) in enumerate(data_loader_acts(dataset, dataset_counter, dataset_act_vecs,  batch_size, wordtoidx)):\n",
    "\n",
    "            batch_size_curr = data.shape[1]\n",
    "            output = model(data, targets,  act_vecs)\n",
    "            loss = criterion(output.view(-1, ntokens), labels.reshape(-1)) \n",
    "            total_loss += loss.item()*batch_size_curr\n",
    "\n",
    "        elapsed = time.time()-start_time\n",
    "\n",
    "    total_loss /= len(dataset)\n",
    "    logger.debug('{} \\tLoss(using ground truths): {:0.7f}\\tTime taken: {:0.1f}s'.format(split, total_loss, elapsed))\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "# stat_cuda('before training')\n",
    "def training(model, args, criterion, optimizer, scheduler, optuna_callback=None):\n",
    "    global best_val_bleu, criteria, best_val_loss_ground\n",
    "    best_model = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_bleus = []\n",
    "\n",
    "    best_val_loss_ground=float(\"inf\")\n",
    "    best_val_bleu=-float(\"inf\")\n",
    "    best_criteria=-float(\"inf\")\n",
    "\n",
    "    logger.debug('At begin of training, Best val loss ground : {:0.7f} Best bleu: {:0.4f}, Best criteria: {:0.4f}'.format(best_val_loss_ground, best_val_bleu, best_criteria))\n",
    "    logger.debug('====> STARTING TRAINING NOW')\n",
    "\n",
    "    val_epoch_freq = 3\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train_epoch(model, epoch, args.batch_size, criterion, optimizer, scheduler)\n",
    "\n",
    "        val_loss_ground = get_loss_nograd(model, epoch, args.batch_size, criterion, 'val')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss_ground)\n",
    "\n",
    "        if val_loss_ground < best_val_loss_ground:\n",
    "            best_val_loss_ground = val_loss_ground\n",
    "            logger.debug('==> New optimum found wrt val loss')\n",
    "            save_model(model, args, 'checkpoint_bestloss.pt',train_loss,val_loss_ground, -1)\n",
    "\n",
    "        # if epoch < 15:\n",
    "        #     save_model(model, args, 'checkpoint.pt',train_loss, val_loss_ground, -1)\n",
    "        #     continue\n",
    "\n",
    "        # for every \"val_epoch_freq\" epochs, evaluate the metrics\n",
    "        if epoch%val_epoch_freq!=0:\n",
    "            save_model(model, args, 'checkpoint.pt', train_loss, val_loss_ground, -1)\n",
    "            continue\n",
    "\n",
    "        _, val_bleu, val_f1entity, matches, successes = evaluate(model, args, val, val_counter, val_hierarchial_actvecs, args.batch_size, criterion, 'val', method='greedy')\n",
    "        val_criteria = val_bleu+0.5*matches+0.5*successes\n",
    "\n",
    "        if optuna_callback is not None:\n",
    "            optuna_callback(epoch/val_epoch_freq, val_criteria) # Pass the score metric on validation set here.\n",
    "\n",
    "        if val_bleu > best_val_bleu:\n",
    "            best_val_bleu = val_bleu\n",
    "            logger.debug('==> New optimum found wrt val bleu')\n",
    "            save_model(model, args, 'checkpoint_bestbleu.pt',train_loss,val_loss_ground, val_bleu)\n",
    "        \n",
    "        if  val_criteria > best_criteria:\n",
    "            best_criteria = val_criteria\n",
    "            best_model = model\n",
    "            logger.debug('==> New optimum found wrt val criteria')\n",
    "            save_model(model, args, 'checkpoint_criteria.pt',train_loss, val_loss_ground, val_bleu)\n",
    "\n",
    "        save_model(model, args, 'checkpoint.pt',train_loss, val_loss_ground, val_bleu)\n",
    "        scheduler.step()\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, args, name, train_loss, val_loss, val_bleu):\n",
    "    checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'embedding_size': args.embedding_size,\n",
    "                    'nhead':args.nhead,\n",
    "                    'nhid': args.nhid,\n",
    "                    'nlayers_e1': args.nlayers_e1,\n",
    "                    'nlayers_e2': args.nlayers_e2,\n",
    "                    'nlayers_d': args.nlayers_d,\n",
    "                    'dropout': args.dropout\n",
    "                 }\n",
    "    if train_loss!=-1:\n",
    "        checkpoint['train_loss']=train_loss\n",
    "    if val_loss!=-1:\n",
    "        checkpoint['val_loss']=val_loss\n",
    "    if val_bleu!=-1:\n",
    "        checkpoint['val_bleu']=val_bleu\n",
    "\n",
    "    logger.debug('==> Checkpointing everything now...in {}'.format(name))\n",
    "    torch.save(checkpoint, args.log_path+name)\n",
    "\n",
    "\n",
    "def load_model(model, checkpoint='checkpoint.pt'):\n",
    "    load_file = checkpoint\n",
    "    if os.path.isfile(load_file):\n",
    "        global best_val_bleu, best_val_loss_ground, criteria\n",
    "        try:\n",
    "            print('Reloading previous checkpoint', load_file)\n",
    "\n",
    "            if not torch.cuda.is_available():\n",
    "                # load dataparallel model into cpu\n",
    "                checkpoint = torch.load(load_file,map_location=lambda storage, loc: storage)\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in checkpoint['model'].items():\n",
    "                    if k[:6]==\"module\":\n",
    "                        name = k[7:] # remove `module.`\n",
    "                    else:\n",
    "                        name=k\n",
    "                    new_state_dict[name] = v\n",
    "                # load params\n",
    "                model.load_state_dict(new_state_dict)\n",
    "\n",
    "            else:\n",
    "                checkpoint = torch.load(load_file)\n",
    "                model.load_state_dict(checkpoint['model'])\n",
    "            #    optimizer.load_state_dict(checkpoint['optim'])\n",
    "\n",
    "            \n",
    "            if(checkpoint.get('val_loss')):\n",
    "                best_val_loss_ground = checkpoint['val_loss']\n",
    "            # else:\n",
    "                # best_val_loss_ground= get_loss_nograd(model, 0, args.batch_size, criterion, 'val')\n",
    "                \n",
    "\n",
    "            if(checkpoint.get('val_bleu')):\n",
    "                best_val_bleu = checkpoint.get('val_bleu')\n",
    "                logger.debug('Valid bleu of Loaded model is: {:0.4f}'.format(best_val_bleu))\n",
    "\n",
    "            logger.debug('Loaded model, Val loss(ground): {:0.8f}'.format(best_val_loss_ground))\n",
    "        except Exception as e:\n",
    "            logger.debug('Loading model error')\n",
    "            logger.debug(e)\n",
    "    else:\n",
    "        best_val_loss_ground = float('inf')\n",
    "        logger.debug('No model to load')\n",
    "    return best_val_loss_ground\n",
    "\n",
    "\n",
    "\n",
    "def name_to_dataset(split):\n",
    "    if split=='train':\n",
    "        return train, train_counter, train_hierarchial_actvecs\n",
    "    if split=='val':\n",
    "        return val, val_counter, val_hierarchial_actvecs\n",
    "    if split=='test':\n",
    "        return test, test_counter, test_hierarchial_actvecs\n",
    "    print('Error')\n",
    "\n",
    "\n",
    "\n",
    "def testing(model, args, criterion, split, method):\n",
    "    data, dataset_counter, dataset_act_vecs = name_to_dataset(split)\n",
    "    if method =='greedy':\n",
    "        return evaluate(model, args, data,dataset_counter, dataset_act_vecs, args.batch_size, criterion, split, method='greedy')\n",
    "    elif method=='beam':\n",
    "        return evaluate(model, args, data, dataset_counter, dataset_act_vecs , args.batch_size, criterion, split, method='beam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split(split, model, args, criterion):\n",
    "    data, dataset_counter, dataset_act_vecs = name_to_dataset(split)\n",
    "    # greedy\n",
    "    evaluate(model, args, data, dataset_counter, dataset_act_vecs, args.batch_size, criterion, split, 'greedy')\n",
    "    # beam 2\n",
    "    # evaluate(model, args, data, dataset_counter, dataset_act_vecs, args.batch_size, criterion, split, 'beam', 2)\n",
    "    # beam 3\n",
    "    # evaluate(model, args, data, dataset_counter, dataset_act_vecs, args.batch_size, criterion, split, 'beam', 3)\n",
    "    # beam 5\n",
    "    # evaluate(model, args, data, dataset_counter, dataset_act_vecs, args.batch_size, criterion, split, 'beam', 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train  dataset is 56778\n",
      "Length of val  dataset is 7374\n",
      "Length of test  dataset is 7372\n",
      "length of vocab:  1505\n"
     ]
    }
   ],
   "source": [
    "# global logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s:%(name)s:%(message)s\")\n",
    "\n",
    "train,train_counter,train_hierarchial_actvecs,train_dialog_files, train_responses = gen_dataset_with_acts('train')\n",
    "val, val_counter, val_hierarchial_actvecs, val_dialog_files, val_responses = gen_dataset_with_acts('val')\n",
    "test, test_counter, test_hierarchial_actvecs, test_dialog_files, test_responses =gen_dataset_with_acts('test')\n",
    "\n",
    "\n",
    "max_sent_len = 50\n",
    "\n",
    "idxtoword, wordtoidx = build_vocab_freqbased(load=False)\n",
    "vocab_size = len(idxtoword)\n",
    "\n",
    "print('length of vocab: ', vocab_size)\n",
    "\n",
    "ntokens=len(wordtoidx)\n",
    "\n",
    "BLEU_calc = BLEUScorer() \n",
    "F1_calc = F1Scorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser() \n",
    "\n",
    "parser.add_argument(\"-embed\", \"--embedding_size\", default=100, type=int, help = \"Give embedding size\")\n",
    "parser.add_argument(\"-heads\", \"--nhead\", default=4, type=int,  help = \"Give number of heads\")\n",
    "parser.add_argument(\"-hid\", \"--nhid\", default=100, type=int,  help = \"Give hidden size\")\n",
    "\n",
    "parser.add_argument(\"-l_e1\", \"--nlayers_e1\", default=3, type=int,  help = \"Give number of layers for Encoder 1\")\n",
    "parser.add_argument(\"-l_e2\", \"--nlayers_e2\", default=3, type=int,  help = \"Give number of layers for Encoder 2\")\n",
    "parser.add_argument(\"-l_d\", \"--nlayers_d\", default=3, type=int,  help = \"Give number of layers for Decoder\")\n",
    "\n",
    "parser.add_argument(\"-d\", \"--dropout\",default=0.2, type=float, help = \"Give dropout\")\n",
    "parser.add_argument(\"-bs\", \"--batch_size\", default=32, type=int, help = \"Give batch size\")\n",
    "parser.add_argument(\"-e\", \"--epochs\", default=30, type=int, help = \"Give number of epochs\")\n",
    "\n",
    "parser.add_argument(\"-model\", \"--model_type\", default=\"SET++\", help=\"Give model name one of [SET++, HIER++]\")\n",
    "\n",
    "# args = parser.parse_args(\"-embed 272 -heads 8 -hid 96 -l_e1 4 -l_e2 2 -l_d 4 -d 0.0868 -bs 1 -e 80 -model HIER++\".split())\n",
    "args = parser.parse_args(\"-embed 175 -heads 7 -hid 91 -l_e1 4 -l_e2 6 -l_d 3 -d 0.071 -bs 1 -e 30 -model HIER++\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=16, dropout=0.071, embedding_size=175, epochs=30, model_type='HIER++', nhead=7, nhid=91, nlayers_d=3, nlayers_e1=4, nlayers_e2=6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-11-14 01:39:08,295] DEBUG:__main__:===> \n",
      "\n",
      "Namespace(batch_size=16, dropout=0.071, embedding_size=175, epochs=30, log_path='running/external/transformer_hier++_001/', model_type='HIER++', nhead=7, nhid=91, nlayers_d=3, nlayers_e1=4, nlayers_e2=6)\n",
      "===>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-11-14 01:39:11,292] DEBUG:__main__:\n",
      "\n",
      "\n",
      "=====>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "putting model on cuda\n",
      "Total number of trainable parameters:  2.934988 M\n",
      "Reloading previous checkpoint running/external/transformer_hier++_001/checkpoint_criteria.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-11-14 01:39:11,502] DEBUG:__main__:Valid bleu of Loaded model is: 27.8801\n",
      "[2020-11-14 01:39:11,503] DEBUG:__main__:Loaded model, Val loss(ground): 1.36594502\n"
     ]
    }
   ],
   "source": [
    "# def run(args, optuna_callback=None):\n",
    "#     global logger \n",
    "\n",
    "if args.model_type==\"SET++\":\n",
    "    log_path ='running/transformer_set++/'\n",
    "elif args.model_type==\"HIER++\":\n",
    "    log_path ='running/external/transformer_hier++_001/'\n",
    "else:\n",
    "    print('Invalid model type')\n",
    "    raise ValueError\n",
    "\n",
    "if not os.path.isdir(log_path[:-1]):\n",
    "    os.makedirs(log_path[:-1])\n",
    "\n",
    "args.log_path = log_path\n",
    "\n",
    "# file logger\n",
    "time_stamp = '{:%d-%m-%Y_%H:%M:%S}'.format(datetime.now())\n",
    "fh = logging.FileHandler(log_path+'train_'+ time_stamp  +'.log', mode='a')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "# console logger - add it when running it on gpu directly to see all sentences\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "logger.debug('===> \\n\\n' + str(args) + '\\n===>\\n\\n')\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # for single device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "max_sent_len = 50\n",
    "\n",
    "ntokens=len(wordtoidx)\n",
    "\n",
    "\n",
    "model = Transformer_acts(ntokens, args.embedding_size, args.nhead, args.nhid, args.nlayers_e1, args.nlayers_e2, args.nlayers_d, args.dropout, args.model_type).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    # using data parallel\n",
    "    model = nn.DataParallel(model, device_ids=[0,1], dim=1)\n",
    "    print('putting model on cuda')\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "print('Total number of trainable parameters: ', sum(p.numel() for p in model.parameters() if p.requires_grad)/float(1000000), 'M')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=4, gamma=0.98)\n",
    "\n",
    "logger.debug('\\n\\n\\n=====>\\n')\n",
    "\n",
    "# best_val_loss_ground = load_model(model, 'checkpoint_criteria.pt')\n",
    "# _ = training(model, args, criterion, optimizer, scheduler, optuna_callback)\n",
    "best_val_loss_ground = load_model(model, args.log_path + 'checkpoint_criteria.pt') #load model with best criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.debug('Testing model\\n')\n",
    "# _,test_bleu ,test_f1 ,test_matches,test_successes = testing(model, args, criterion, 'test', 'greedy')\n",
    "# logger.debug('==>Test \\tBleu: {:0.3f}\\tF1-Entity {:0.3f}\\tInform {:0.3f}\\tSuccesses: {:0.3f}'.format(test_bleu, test_f1, test_matches, test_successes))\n",
    "# logger.debug('Test critiera: {}'.format(test_bleu+0.5*(test_matches+test_successes)))\n",
    "\n",
    "# # To get greedy, beam(2,3,5) scores for val, test \n",
    "# test_split('val', model, args, criterion)\n",
    "test_split('test', model, args, criterion)\n",
    "\n",
    "_,val_bleu ,_,val_matches,val_successes = testing(model, args, criterion, 'val', 'greedy')\n",
    "print(val_bleu+0.5*(val_matches+val_successes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
