{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging.handlers\n",
    "import os\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from MultiWOZ import get_batch\n",
    "from evaluator import evaluateModel\n",
    "from tools import *\n",
    "from transformer.Transformer import RespGenerator, UncertaintyLoss\n",
    "\n",
    "\n",
    "def parse_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--option', type=str, default=\"train\", help=\"whether to train or test the model\", choices=['train', 'test', 'postprocess'])\n",
    "    parser.add_argument('--emb_dim', type=int, default=128, help=\"the embedding dimension\")\n",
    "    parser.add_argument('--dropout', type=float, default=0.2, help=\"dropout rate\")\n",
    "    parser.add_argument('--resume', action='store_true', default=False, help=\"whether to resume previous run\")\n",
    "    parser.add_argument('--batch_size', type=int, default=3, help=\"train/dev/test batch size\")\n",
    "    parser.add_argument('--model', type=str, default=\"model\", help=\"path to save or load models\")\n",
    "    parser.add_argument('--data_dir', type=str, default='data', help=\"data dir\")\n",
    "    parser.add_argument('--beam_size', type=int, default=2, help=\"beam size of act/response generator\")\n",
    "    parser.add_argument('--max_seq_length', type=int, default=50, help=\"max input length\")\n",
    "    parser.add_argument('--ngram', type=int, default=3, help=\"avoid n gram repeatness\")\n",
    "    parser.add_argument('--layer_num', type=int, default=3, help=\"transformer layer num\")\n",
    "    parser.add_argument('--evaluate_every', type=int, default=5, help=\"checkpoints\")\n",
    "    parser.add_argument('--head', type=int, default=4, help=\"head num for transformer\")\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-3, type=float, help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--output_file\", default='output', type=str, help=\"path to save generated act/response\")\n",
    "    parser.add_argument(\"--non_delex\", default=False, action=\"store_true\", help=\"non delex testing\")\n",
    "    parser.add_argument(\"--hist_num\", default=0,type=int, help=\"turn num of history\")\n",
    "    parser.add_argument('--log', type=str, default='log', help=\"log file\")\n",
    "    \n",
    "    parser.add_argument('--nlayers_e', type=int, default=-1, help=\"transformer-encoder layer num (half utt, half context)\")\n",
    "    parser.add_argument('--nlayers_d', type=int, default=-1, help=\"transformer-decoder layer num\")\n",
    "\n",
    "    parser.add_argument('--act_source',  type=str, choices=[\"pred\", \"bert\",'groundtruth'], default='pred', help=\"action source for validate/test\")\n",
    "    parser.add_argument('--seed', type=int, default=1, help=\"random seed for initialization\")\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_args(\"--option train --model modelAct/ --batch_size 2 --max_seq_length 50 --act_source pred --seed 0 --learning_rate 1e-4\".split())\n",
    "    if args.nlayers_e == -1:\n",
    "        args.nlayers_e = args.layer_num\n",
    "    if args.nlayers_d == -1:\n",
    "        args.nlayers_d = args.layer_num\n",
    "    return args\n",
    "\n",
    "\n",
    "args = parse_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args(\"--option train --model model/ --batch_size 384 --max_seq_length 50 --act_source bert --seed 0 --learning_rate 1e-4\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(act_source='pred', batch_size=2, beam_size=2, data_dir='data', dropout=0.2, emb_dim=128, evaluate_every=5, head=4, hist_num=0, layer_num=3, learning_rate=0.0001, log='log', max_seq_length=50, model='modelAct/', ngram=3, nlayers_d=3, nlayers_e=3, non_delex=False, option='train', output_file='output', resume=False, seed=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if args.option == 'train':\n",
    "    if not os.path.exists(args.model):\n",
    "        os.makedirs(args.model)\n",
    "    args.log = os.path.join(args.model, f'train_{timestr}.log')\n",
    "elif args.option == 'test':\n",
    "    dir = os.path.dirname(args.model)\n",
    "    args.log = os.path.join(dir, f'test_{timestr}.log')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "handler1 = logging.StreamHandler()\n",
    "handler2 = logging.FileHandler(filename=args.log)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler1.setLevel(logging.WARNING)\n",
    "handler2.setLevel(logging.DEBUG)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s %(name)s %(levelname)s %(message)s\")\n",
    "handler1.setFormatter(formatter)\n",
    "handler2.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(handler1)\n",
    "logger.addHandler(handler2)\n",
    "\n",
    "logger.info(\"========== MARCO HIER =======\")\n",
    "logger.info(args)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    numpy.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "if args.seed != 0:\n",
    "    setup_seed(args.seed)\n",
    "else:\n",
    "    logger.info(\"no random seed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}/vocab.json\".format(args.data_dir), 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "\n",
    "act_ontology = Constants.act_ontology\n",
    "\n",
    "vocab, ivocab = vocabulary['vocab'], vocabulary['rev']\n",
    "tokenizer = Tokenizer(vocab, ivocab, False)\n",
    "\n",
    "with open(\"{}/act_vocab.json\".format(args.data_dir), 'r') as f:\n",
    "    act_vocabulary = json.load(f)\n",
    "\n",
    "act_vocab, act_ivocab = act_vocabulary['vocab'], act_vocabulary['rev']\n",
    "act_tokenizer = Tokenizer(act_vocab, act_ivocab, False)\n",
    "\n",
    "logger.info(\"Loading Vocabulary of {} size\".format(tokenizer.vocab_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "checkpoint_file = args.model\n",
    "\n",
    "if 'train' in args.option:\n",
    "    *train_examples, _ = get_batch(args.data_dir, 'train', tokenizer, act_tokenizer, args.max_seq_length)\n",
    "    train_data = TensorDataset(*train_examples)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.batch_size)\n",
    "    *val_examples, val_id = get_batch(args.data_dir, 'val', tokenizer, act_tokenizer, args.max_seq_length)\n",
    "    dialogs = json.load(open('{}/val.json'.format(args.data_dir)))\n",
    "    gt_turns = json.load(open('{}/val_reference.json'.format(args.data_dir)))\n",
    "elif 'test' in args.option or 'postprocess' in args.option:\n",
    "    *val_examples, val_id = get_batch(args.data_dir, 'test', tokenizer, act_tokenizer, args.max_seq_length)\n",
    "    dialogs = json.load(open('{}/test.json'.format(args.data_dir)))\n",
    "    if args.non_delex:\n",
    "        gt_turns = json.load(open('{}/test_reference_nondelex.json'.format(args.data_dir)))\n",
    "    else:\n",
    "        gt_turns = json.load(open('{}/test_reference.json'.format(args.data_dir)))\n",
    "\n",
    "eval_data = TensorDataset(*val_examples)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU_calc = BLEUScorer()\n",
    "F1_calc = F1Scorer()\n",
    "\n",
    "best_BLEU = 0\n",
    "\n",
    "weighted_loss_func = UncertaintyLoss(2)\n",
    "weighted_loss_func.to(device)\n",
    "\n",
    "resp_generator = RespGenerator(vocab_size=tokenizer.vocab_len,\n",
    "                               act_vocab_size=act_tokenizer.vocab_len,\n",
    "                               d_word_vec=args.emb_dim,\n",
    "                               act_dim=Constants.act_len,\n",
    "                               nlayers_e=args.nlayers_e, \n",
    "                               nlayers_d=args.nlayers_d,\n",
    "                               d_model=args.emb_dim,\n",
    "                               n_head=args.head,\n",
    "                               dropout=args.dropout)\n",
    "\n",
    "resp_generator.to(device)\n",
    "\n",
    "bce_loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "bce_loss_func.to(device)\n",
    "\n",
    "ce_loss_func = torch.nn.CrossEntropyLoss(ignore_index=Constants.PAD)\n",
    "ce_loss_func.to(device)\n",
    "\n",
    "\n",
    "label_list = Constants.functions + Constants.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "# numpy.set_printoptions(threshold=sys.maxsize)\n",
    "numpy.set_printoptions(threshold=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.option == 'train':\n",
    "    resp_generator.train()\n",
    "    if args.resume:\n",
    "        logger.info(\"Reloaing the encoder and act_generator from {}\".format(checkpoint_file))\n",
    "\n",
    "    logger.info(\"Start Training with {} batches\".format(len(train_dataloader)))\n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, list(resp_generator.parameters()) + list(weighted_loss_func.parameters())), betas=(0.9, 0.98), eps=1e-09)\n",
    "\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[50, 100, 150, 200], gamma=0.5)\n",
    "\n",
    "    alpha = 0.1\n",
    "    for epoch in range(51):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, action_masks, rep_in, resp_out, belief_state,\\\n",
    "                bert_act_seq, act_in, act_out, all_label, act_input_mask,\\\n",
    "                resp_input_mask, utt_loc, *_ = batch\n",
    "\n",
    "            resp_generator.zero_grad()\n",
    "\n",
    "            # act loss\n",
    "            logits, _, act_vecs = resp_generator.act_forward(\n",
    "                tgt_seq=act_in, src_seq=input_ids, bs=belief_state, input_mask=act_input_mask,\n",
    "                 utt_loc=utt_loc\n",
    "            )\n",
    "\n",
    "            loss1 = ce_loss_func(\n",
    "                logits.contiguous().view(logits.size(0) * logits.size(1), -1).contiguous(),\n",
    "                act_out.contiguous().view(-1))\n",
    "\n",
    "            # response loss\n",
    "    #         print(\"input mask:\", resp_input_mask.shape)\n",
    "            resp_logits = resp_generator.resp_forward(tgt_seq=rep_in, src_seq=input_ids, act_vecs=act_vecs,\n",
    "                                                      act_mask=action_masks, input_mask=resp_input_mask, utt_loc=utt_loc)\n",
    "\n",
    "            loss2 = ce_loss_func(\n",
    "                resp_logits.contiguous().view(resp_logits.size(0) * resp_logits.size(1), -1).contiguous(),\n",
    "                resp_out.contiguous().view(-1))\n",
    "\n",
    "            # overall loss\n",
    "            if epoch < 10:\n",
    "                loss = loss1\n",
    "            else:\n",
    "                loss = weighted_loss_func(loss1, loss2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(\"epoch {} \\tstep {} training \\ttotal_loss {:.6f} \\tact_loss {:.6f} \\tresp_loss {:.6f}\".format(\n",
    "                    epoch, step, loss.item(), loss1.item(), loss2.item()))\n",
    "\n",
    "        alpha = min(1, alpha + 0.1 * epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "        if loss2.item() < 3.0 and loss1.item() < 3.0 and epoch > 0 and epoch % args.evaluate_every == 0:\n",
    "        # if True:\n",
    "            logger.info(\"start evaluating BLEU on validation set\")\n",
    "            resp_generator.eval()\n",
    "            # Start Evaluating after each epoch\n",
    "            model_turns = {}\n",
    "            TP, TN, FN, FP = 0, 0, 0, 0\n",
    "            for batch_step, batch in enumerate(eval_dataloader):\n",
    "                all_pred = []\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, action_masks, rep_in, resp_out, belief_state, \\\n",
    "                    bert_act_seq, act_in, act_out, all_label, act_input_mask, \\\n",
    "                    resp_input_mask, utt_loc, *_ = batch\n",
    "\n",
    "                if args.act_source == 'bert':\n",
    "                    act_in = bert_act_seq\n",
    "\n",
    "                elif args.act_source == 'pred':\n",
    "                    hyps, act_logits = resp_generator.act_translate_batch(input_mask=act_input_mask, bs=belief_state, \\\n",
    "                                                                          src_seq=input_ids, utt_loc=utt_loc, n_bm=args.beam_size,\n",
    "                                                                          max_token_seq_len=Constants.ACT_MAX_LEN)\n",
    "                    for hyp_step, hyp in enumerate(hyps):\n",
    "                        pre1 = [0] * Constants.act_len\n",
    "                        if len(hyp) < Constants.ACT_MAX_LEN:\n",
    "                            hyps[hyp_step] = list(hyps[hyp_step]) + [Constants.PAD] * (Constants.ACT_MAX_LEN - len(hyp))\n",
    "                        for w in hyp:\n",
    "                            if w not in [Constants.PAD, Constants.EOS]:\n",
    "                                pre1[w - 3] = 1\n",
    "                        all_pred.append(pre1)\n",
    "                    all_pred = torch.Tensor(all_pred)\n",
    "                    all_label = all_label.cpu()\n",
    "\n",
    "                    TP, TN, FN, FP = obtain_TP_TN_FN_FP(all_pred, all_label, TP, TN, FN, FP)\n",
    "                    act_in = torch.tensor(hyps, dtype=torch.long).to(device)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                _, _, act_vecs = resp_generator.act_forward(tgt_seq=act_in, src_seq=input_ids, bs=belief_state,\n",
    "                                                            input_mask=act_input_mask, utt_loc=utt_loc)\n",
    "                action_masks = act_in.eq(Constants.PAD) + act_in.eq(Constants.EOS)\n",
    "                resp_hyps = resp_generator.resp_translate_batch(bs=belief_state, act_vecs=act_vecs,\n",
    "                                                                act_mask=action_masks, input_mask=resp_input_mask,\n",
    "                                                                src_seq=input_ids, utt_loc=utt_loc, n_bm=args.beam_size,\n",
    "                                                                max_token_seq_len=40)\n",
    "                for hyp_step, hyp in enumerate(resp_hyps):\n",
    "                    pred = tokenizer.convert_id_to_tokens(hyp)\n",
    "                    file_name = val_id[batch_step * args.batch_size + hyp_step]\n",
    "                    if file_name not in model_turns:\n",
    "                        model_turns[file_name] = [pred]\n",
    "                    else:\n",
    "                        model_turns[file_name].append(pred)\n",
    "\n",
    "            precision = TP / (TP + FP + 0.001)\n",
    "            recall = TP / (TP + FN + 0.001)\n",
    "            F1 = 2 * precision * recall / (precision + recall + 0.001)\n",
    "            print(\"precision is {:.6f} recall is {:.6f} F1 is {:.6f}\".format(precision, recall, F1))\n",
    "            logger.info(\"precision is {:.6f} recall is {:.6f} F1 is {:.6f}\".format(precision, recall, F1))\n",
    "            BLEU = BLEU_calc.score(model_turns, gt_turns)\n",
    "            inform, request = evaluateModel(model_turns)\n",
    "            score = (inform + request) / 2.0 + 100 * BLEU\n",
    "            print(\"{} epoch, Validation BLEU {:.4f}, inform {:.2f}, request {:.2f}, score {:.2f}\".format(epoch, BLEU, inform, request, (inform + request) / 2 + 100 * BLEU))\n",
    "            logger.info(\"{} epoch, Validation BLEU {:.4f}, inform {:.2f}, request {:.2f}, score {:.2f}\".format(epoch, BLEU, inform, request, (inform + request) / 2 + 100 * BLEU))\n",
    "            if score > best_BLEU: # Use total criteria\n",
    "                save_name = 'inform-{:.2f}-request-{:.2f}-bleu-{:.4f}-seed-{}'.format(inform, request, BLEU, args.seed)\n",
    "                torch.save(resp_generator.state_dict(), os.path.join(checkpoint_file, save_name))\n",
    "                best_BLEU = score\n",
    "                resp_file = os.path.join(args.output_file, 'resp_pred.json')\n",
    "                with open(resp_file, 'w') as fp:\n",
    "                    model_turns = OrderedDict(sorted(model_turns.items()))\n",
    "                    json.dump(model_turns, fp, indent=2)\n",
    "            resp_generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_generator.post_word_emb.pe.gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.option == \"test\" or args.option == \"train\":\n",
    "    if args.option == \"train\":\n",
    "        logger.info(\"Test after training...\")\n",
    "        args.model = os.path.join(args.model, save_name)\n",
    "        checkpoint_file = args.model # Auto: the best criteria file\n",
    "    resp_generator.load_state_dict(torch.load(args.model))\n",
    "    logger.info(\"Loading model from {}\".format(checkpoint_file))\n",
    "    resp_generator.eval()\n",
    "    # Start Evaluating after each epoch\n",
    "    model_turns = {}\n",
    "    act_turns={}\n",
    "    TP, TN, FN, FP = 0, 0, 0, 0\n",
    "    example_success={}\n",
    "    for batch_step, batch in enumerate(eval_dataloader):\n",
    "        all_pred = []\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, action_masks, rep_in, resp_out, belief_state, bert_act_seq, act_in, act_out, all_label, \\\n",
    "        act_input_mask, resp_input_mask, utt_loc, *_ = batch\n",
    "\n",
    "        if args.act_source == 'bert':\n",
    "            act_in = bert_act_seq\n",
    "        elif args.act_source == 'pred':\n",
    "            hyps, act_logits = resp_generator.act_translate_batch(input_mask=act_input_mask, bs=belief_state, \\\n",
    "                                                                  src_seq=input_ids, utt_loc=utt_loc, n_bm=args.beam_size,\n",
    "                                                                  max_token_seq_len=Constants.ACT_MAX_LEN)\n",
    "            for hyp_step, hyp in enumerate(hyps):\n",
    "                pre1 = [0] * Constants.act_len\n",
    "                for w in hyp:\n",
    "                    if w not in [Constants.PAD, Constants.EOS]:\n",
    "                        pre1[w - 3] = 1\n",
    "                if len(hyp) < Constants.ACT_MAX_LEN:\n",
    "                    hyps[hyp_step] = list(hyps[hyp_step]) + [Constants.PAD] * (Constants.ACT_MAX_LEN - len(hyp))\n",
    "                all_pred.append(pre1)\n",
    "                file_name = val_id[batch_step * args.batch_size + hyp_step]\n",
    "                if file_name not in act_turns:\n",
    "                    act_turns[file_name] = [pre1]\n",
    "                else:\n",
    "                    act_turns[file_name].append(pre1)\n",
    "\n",
    "            all_pred=torch.Tensor(all_pred)\n",
    "            all_label=all_label.cpu()\n",
    "            TP, TN, FN, FP = obtain_TP_TN_FN_FP(all_pred, all_label, TP, TN, FN, FP)\n",
    "\n",
    "            act_in = torch.tensor(hyps, dtype=torch.long).to(device)\n",
    "        else:\n",
    "            pass\n",
    "        _, _, act_vecs = resp_generator.act_forward(tgt_seq=act_in, src_seq=input_ids, bs=belief_state,\n",
    "                                                    input_mask=act_input_mask, utt_loc=utt_loc,)\n",
    "        action_masks = act_in.eq(Constants.PAD) + act_in.eq(Constants.EOS)\n",
    "        resp_hyps = resp_generator.resp_translate_batch(bs=belief_state, act_vecs=act_vecs, act_mask=action_masks,\n",
    "                                                        input_mask=resp_input_mask,\n",
    "                                                        src_seq=input_ids, utt_loc=utt_loc, n_bm=args.beam_size,\n",
    "                                                        max_token_seq_len=40,gram_num=args.ngram)\n",
    "\n",
    "        for hyp_step, hyp in enumerate(resp_hyps):\n",
    "            pred = tokenizer.convert_id_to_tokens(hyp)\n",
    "            file_name = val_id[batch_step * args.batch_size + hyp_step]\n",
    "            if file_name not in model_turns:\n",
    "                model_turns[file_name] = [pred]\n",
    "            else:\n",
    "                model_turns[file_name].append(pred)\n",
    "\n",
    "    precision = TP / (TP + FP + 0.001)\n",
    "    recall = TP / (TP + FN + 0.001)\n",
    "    F1 = 2 * precision * recall / (precision + recall + 0.001)\n",
    "    print(\"precision is {:.6f} recall is {:.6f} F1 is {:.6f}\".format(precision, recall, F1))\n",
    "    logger.info(\"precision is {:.6f} recall is {:.6f} F1 is {:.6f}\".format(precision, recall, F1))\n",
    "    BLEU = BLEU_calc.score(model_turns, gt_turns)\n",
    "    inform, request = evaluateModel(model_turns, example_success)\n",
    "    print(\"Test BLEU {:.4f}, inform {:.2f}, request {:.2f}, score {:.2f}\".format(BLEU, inform, request, (inform + request) / 2 + 100 * BLEU))\n",
    "    logger.info(\"Test BLEU {:.4f}, inform {:.2f}, request {:.2f}, score {:.2f}\".format(BLEU, inform, request, (inform + request) / 2 + 100 * BLEU))\n",
    "\n",
    "    resp_file = os.path.join(args.output_file, 'resp_pred.json')\n",
    "    with open(resp_file, 'w') as fp:\n",
    "        model_turns = OrderedDict(sorted(model_turns.items()))\n",
    "        json.dump(model_turns, fp, indent=2)\n",
    "\n",
    "    act_file = os.path.join(args.output_file, 'act_pred.json')\n",
    "    with open(act_file, 'w') as fp:\n",
    "        act_turns = OrderedDict(sorted(act_turns.items()))\n",
    "        json.dump(act_turns, fp, indent=2)\n",
    "\n",
    "    with open('output/example_statistic.json','w') as f:\n",
    "        json.dump(example_success,f)\n",
    "\n",
    "    save_name = 'test-inform-{:.2f}-request-{:.2f}-bleu-{:.4f}'.format(inform, request, BLEU)\n",
    "    torch.save(resp_generator.state_dict(), os.path.join('model', save_name))\n",
    "\n",
    "elif args.option == \"postprocess\":\n",
    "\n",
    "    resp_file = os.path.join(args.output_file, 'resp_pred.json')\n",
    "    with open(resp_file, 'r') as f:\n",
    "        model_turns = json.load(f)\n",
    "\n",
    "    success_rate = nondetokenize(model_turns, dialogs)\n",
    "    BLEU = BLEU_calc.score(model_turns, gt_turns)\n",
    "    print(BLEU)\n",
    "\n",
    "    resp_file = os.path.join(args.output_file, 'resp_non_delex_pred.json')\n",
    "    with open(resp_file, 'w') as fp:\n",
    "        model_turns = OrderedDict(sorted(model_turns.items()))\n",
    "        json.dump(model_turns, fp, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
